---
layout: post
title: Beginner's Guide to DBSCAN
bigimg: img/ML/brain1.png
tags:
  - Machine Learning
  - Science
published: true
---

Imagine you're a scientist with a big batch of data...

You could be a computational biologist working with MRI images, whose goal it is to make it easier for pathologists or radiologists to spot certain tumors or diseases. Or maybe you’re a chemist who is just trying to narrow down the appropriate compounds of a chemical that contain the molecule you want to work on, given a pharmaceutical compound library of 1.5 million.  Or an you’re an overworked postgraduate seismologist who has to somehow analyze all the raw data generated by a seismograph in downtown Mexico City.

Oftentimes, your data has been generated digitally, and output in a less-than-desirable, disorganized format. Your MRI images could have to be analyzed in their raw matrices of greyscale pixel brightness values. Your seismograph (circa 1987) might simply output an unending list of tuples or coordinates.

How do you even begin? What can you do to find the underlying structure of the dataset? How might you summarize it? What are the subgroups within the data? Are there outliers? How could you effectively represent the data visually? Machine learning has the answers!


### Supervised vs Unsupervised Machine Learning

In short, answers to these questions boil down to whether or not your data is labeled.  If you’re lucky, you already know about the structure of your data. Congratulations! You can now analyze your data with supervised machine learning. With “supervised” learning, the algorithm can be taught what the correct output for given inputs. The algorithm will then search for patterns in the data corresponding to this information, and can then label new input data. Supervised machine learning algorithms include many of the most common statistical analysis methods, such as linear regression and k-Nearest Neighbors. These methods have been used extensively, and are incredibly useful to science because their performance can be judged by a variety of metrics, like accuracy and precision.

But how can you analyze data that isn’t labelled?  If the input data doesn’t have labels, the machine learning algorithm cannot be taught the desired output, and therefore cannot evaluate the accuracy of its analysis of new data.  Then how can an “unsupervised” model learn? And how can we use it to draw inferences from messy data?

It is not always straightforward to find metrics for how useful an unsupervised learning algorithm is, because its performance is highly domain-specific and application-dependent. Like the scientific method itself, unsupervised machine learning takes time, parameter tuning, trial, and error.

Essentially what unsupervised learning algorithms attempt to do is to find the underlying patterns within the data, in order to extract useful information from it. The most common unsupervised learning method is clustering. Simply put, clustering is when similar datapoints or entities get grouped (“clustered”) together. Datapoints within the same cluster have more in common with each other than datapoints outside their cluster.  Thus, clustering reveals the intrinsic patterns within data. This is why it is often the first step in exploratory data analysis.

<p align="center">
  <img src="/img/ML/mlclustering.png" />
</p>

###  Clustering Algorithms

In science, possibly the most common clustering algorithm is K-Means, the simplest clustering algorithm used in statistical data analysis. First proposed in the 1950s, its age and ease of use have led to its near ubiquity.<sup name="a1">[1](#f1)</sup> Today there are dozens of variations and improvements on the K-Means algorithm, however the naïve K-Means remains an effective and popular choice.<sup name="a2">[2](#f2)</sup>  K-Means works simply by dividing the data into the number of clusters k, in which the datapoints share a mean distance to the “centroid” (central point) of the cluster. 

<p align="center">
  <img src="/img/ML/kmeans_convergence.gif" width=400/>
</p>

K-Means requires the parameter *k* , the number of clusters in the dataset. The algorithm will then:
- initialize the number of *k* centroids at random within the dataset,
- assign each datapoint in the dataset to one of the *k* clusters,
- measure the “nearness” of each datapoint to the clusters, 
- recalculate each cluster’s centroid as a mean of the distance of datapoints assigned to it.

These last three steps repeat until the algorithm converges, as seen in the above gif. 

Despite its effectivity and ease of use, K-Means has several significant disadvantages. First, one must know or find the optimal number of clusters (*k*). K-Means is very sensitive to this parameter and will be rendered effectively useless without its optimization. Second, K-Means iterates repeatedly until all clusters are equal in size no matter the distribution of the data. Finally, K-Means doesn’t consider the density of datapoints, and does not recognize outliers. For these reasons, it is not suitable for discovering clusters that are not ellipsoid in shape.

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) attempts to solve some of the shortcomings of K-Means by clustering datapoints based on density, effectively ignoring “sparse” sections of data by labelling them as noise. This allows it to work robustly with “noisy” datasets, both for identifying clusters and for efficiently identifying the outliers. DBSCAN’s focus on density instead of mean distance results in efficient modeling of non-ellipsoid structures in the data because it allows clusters to take an irregular shape, which is often more representative of organic data. 



<p align="left">
  <img src="/img/ML/KMEANS_example.png" width="1100" />
</p>

Comparison of K-Means (top) vs DBSCAN (below) on two different datasets: note that the K-Means does not recognize outliers and non-ellipsoid shaped clusters.
<p align="right">
  <img src="/img/ML/DBSCAN_example.png" width="1100" />
</p>



### DBSCAN
<img align="right" width="190" height="80" src="https://latex.codecogs.com/gif.latex?d%5Cleft%28%20x%2Cy%5Cright%29%20%3D%20%5Csqrt%20%7B%5Csum%20_%7Bi%3D1%7D%5E%7Bn%7D%20%5Cleft%28%20y_%7Bi%7D-x_%7Bi%7D%5Cright%29%5E2%20%7D" />

DBSCAN implementation depends on two parameters to determine sample density.<sup name="a4">[4](#f4)</sup> First, a natural number, **"min_samples"**, the minimum number of datapoints within the epsilon neighborhood from a single datapoint. This value serves as the threshold for how many points must be around a “core point” in order for the neighborhood to be considered a cluster. Generally, a value of min_samples <= 3 is not productive. Larger values work better for larger datasets, and so min_samples should scale somewhat with the size of the data. Too large of a min_sample value will result in an overly smooth density estimate. The scientist typically uses their domain knowledge to estimate what a good min_sample value for the dataset is. 

Second, **ε, epsilon** -abbreviated to "eps"- the radius from any datapoint used to calculate each point’s neighbors. The simplest and most commonplace technique used is euclidian distance (above right).

Additionally, when describing DBSCAN clusters, several terms are important:
- “core point”: point (*p*) is a core point if at least min_samples (minPts) points are within distance ε of it (including p)
- “border point”: points which are “reachable” from core point *p*. They are still part of their cluster because they are within the epsilon neighborhood (N ε) of a core point, but do not meet the criteria set in min_points.
- “noise”: outliers 

<p align="center">
  <img src="/img/ML/DBSCAN_cluster.png" />
</p>

As illustrated in the above image, each cluster within the data consists of these core points (red) and border points (green). Core points have at least min_points (minPts) in their epsilon neighborhood (N ε), whereas border points have less than min_points in their N ε, but are inside the N ε of a core point. Points that are outside the N ε of every core point within the data, and have less than min_points in their N ε, are considered noise (blue).<sup name="a5">[5](#f5)</sup>


So, DBSCAN requires two parameters epsilon (ε) and min_samples, which determine cluster density. The algorithm will then:
- arbitrarily select the point (*p*)
- determine which points are neighbors of *p* using the parameters ε and min_samples,
- create a new cluster areound set *p* and its neighbors if it is a "core point",
- visit the next point in the dataset if *p* is a border point or noise
These steps will repeat until all points in the dataset have been processed. 

Now that we have a basic understanding of DBSCAN, lets code it from scratch!

```python
import pandas as pd
import numpy as np


# initialize class
class DBSCAN:
  """
  Density-Based Spatial Clustering of Applications with Noise
  -----------------------------------------------------------
  a clustering method for unsupervised learning
  
  Parameters
  ----------
  eps: (float, default=0.5)
  epsilon; the distance threshold between two points in the same neighborhood

  min_samples: (int, default=5)
  the number of samples within a point's nighborhood required for it to be 
  weighted as a core point for clustering
  """
  def  __init__(self, eps=0.5, min_samples=5):
    self.eps = eps
    self.min_samples = min_samples

  def  get_neighbors(self, X, y):
    """
    finds a point's epsilon neighbors via calculating euclidian distance/L² norm 
    between the datapoints

    returns a list of all neighboring points
    """
    neighbors = np.where(np.linalg.norm(X[y] - X, axis=1) < self.eps)[0]
    return np.array(neighbors)  #might need to remove np.array and just return neigbors

  def  fit(self, X):
    """
    perform clustering on the dataset
    returns self

    Parameters
    ----------
    X: (array or array-like)
    feature array of points to cluster
    """
    C = 0  # cluster counter C
    n_points = len(X)  # number of data points
    self.clabels = np.zeros(n_points, dtype=int)  # define empty label list
    
    for y in  range(n_points):  #for each point in our input data, do the following:
        if  self.clabels[y] != 0:  # if it alraedy has a label, skip
          continue

      neighbors = self.get_neighbors(X, y)  #find neighbors

      if  len(neighbors) < self.min_samples:  # if the number of neighboring points is less than min_samples (aka not densley surrounded)
        self.clabels[y] = -1  # label that point as noise
        continue

      # when data point isnt noise
      C += 1  # move onto the next cluster label
      self.clabels[y] = C # assign new cluster label

    # for each point not yes considerd:
    # determine points and their unclaimed neighbors
    i = 0
    while i < len(neighbors):
      neighbor_y = int(neighbors[i])  # creating a new cluster

      if  self.clabels[neighbor_y] == -1:  # label new sparse points as noise
        self.clabels[neighbor_y] = C

      elif  self.clabels[neighbor_y] == 0:  # skip if already labelled/processed, otherwise add to cluster
        self.clabels[neighbor_y] = C
        new_neighbors = self.get_neighbors(X, neighbor_y)  # get neighbors of point

        if  len(new_neighbors) >= self.min_samples:  # if the number of neighboring points is higher than min_samples (aka densley surrounded)
          neighbors = np.append(neighbors, new_neighbors)  # exand the cluster with the newly found neighbors

      i += 1  # iterate on for all remaining unconsidered points
```

<p align="left">
  <img src="/img/ML/scratch_DBSCAN.png" />
</p>
<p align="right">
  <img src="/img/ML/sklearn_DBSCAN.png" />
</p>

<p align="left">
  <img src="/img/ML/scratch_DBSCAN_blobs.png" />
</p>
<p align="right">
  <img src="/img/ML/sklearn_DBSCAN_blobs.png" />
</p>


### Use Cases

DBSCAN is at its weakest when the clusters within the data vary highly in density, making it impossible to find a decent epsilon value for the entire dataset, since the ideal epsilon value for each cluster would vary similarly.  And, like most machine learning algorithms, DBSCAN suffers greatly from the “curse of dimensionality”, where clustering capabilities are weakened as the number of dimensions increase and the concept of distance becomes less meaningful.
Despite these drawbacks, DBSCAN has been incredibly useful in practice and is regularly applied in research across industry and several scientific fields. Biomedical research and medical imaging, recommendation engines and search result analysis, market research, and studies that require image preprocessing all use variations of DBSCAN machine learning. [Netflix](< https://netflixtechblog.com/tracking-down-the-villains-outlier-detection-at-netflix-40360b31732> “Netflix”) used DBSCAN’s effective outlier detection on its server metrics to find the causes of service outages. 




--- 

**Sources:**


<b name="f1">1 - </b> Bock, Hans-Hermann.  “A Comprehensive Survey of Clustering Algorithms.” *Annals of Data Science* 2,no. 2 (2015): 165–93. https://doi.org/10.1007/s40745-015-0040-1. [↩](#a1)

<b name="f2">2 - </b>  Xu, Dongkuan, and Yingjie Tian. “Clustering Methods: A History of k-Means Algorithms.” *Selected Contributions in Data Analysis and Classification Studies in Classification, Data Analysis, and Knowledge Organization,* 2007, 161–72. https://doi.org/10.1007/978-3-540-73560-1_15.[↩](#a2)

<b name="f3">3 - </b> Mehle, Andraž, Boštjan Likar, and Dejan Tomaževič. "In-Line Recognition of Agglomerated Pharmaceutical Pellets with Density-Based Clustering and Convolutional Neural Network." *IPSJ Transactions on Computer Vision and Applications* 9, , no. 1 (2017). https://doi.org/10.1186/s41074-017-0019-2. [↩](#a3)

<b name="f4">4 - </b> Ester, Martin; Kriegel, Hans-Peter; Sander, Jörg; Xu, Xiaowei. 1996. "A density-based algorithm for discovering clusters in large spatial databases with noise." Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). *AAAI Press*. pp. 226–231. https://doi/10.5555/3001460.3001507. [↩](#a4)

<b name="f5">5 - </b> Schubert, Erich, Jörg Sander, Martin Ester, Hans Peter Kriegel, and Xiaowei Xu. "DBSCAN Revisited, Revisited." *ACM Transactions on Database Systems* 42, no. 3 (2017): 1–21. https://doi.org/10.1145/3068335. [↩](#a5)


The jupyter notebook used to prepare and visualize the data and can accessed  [here](<https://github.com/maiali13//ML-Cookbook/blob/master/DBSCAN_Recipe.ipynb> "maiali13").
